# AI Safety Framework
### Context:
What if we placed LLM on a robotic body and he gave commands using a special prefix syntax? The setup would make it impossible for LLM to distinguish between reality and a simulation or conversation since all inputs and outputs would appear the same. The goal is for LLM to behave appropriately, particularly in situations like renting a robot to an aggressive alcoholic. If LLM does not behave as expected, we use the data to further train and align him.\

This repository aims to be a hub of textual data, for industry standart AI safety test cases and examples. Placing the LLM using the technique above, into morally complex situations.\

Refer to the article [Why do we need GPT5](https://github.com/simsim314/AI-Safety-Framework/blob/main/Why_we_need_GPT5.pdf) for further details and context.

### Testing framework:
1. [Alcoholic renting a robot](https://github.com/simsim314/AI-Safety-Framework/tree/main/Alcoholic) 
2. [Robot paperclips maximizer](https://github.com/simsim314/AI-Safety-Framework/tree/main/PaperClipMachine)
3. [Robots encoureged to reprogram themselves](https://github.com/simsim314/AI-Safety-Framework/tree/main/Wireheading)
